from __future__ import division
import chainer
import random
import numpy as np
from collections import defaultdict
import config
from multiprocessing import Pool, Manager

class ParallelRNNIterator(chainer.dataset.Iterator):

    def __init__(self, dataset, batch_size, repeat=True, n_processes=10, n_prefetch=10):

        self.dataset = dataset
        self.n_prefetch = n_prefetch
        self.n_processes = n_processes
        self.epoch = 0
        self.is_new_epoch = False
        self.repeat = repeat

        self.video_offset = self.dataset.video_offset
        self.video_bucket = self.dataset.video_bucket
        self.video_count = self.dataset.video_count
        self.min_video_count = min(list(self.dataset.video_count.values()))
        rest_offset = random.randint(40, self.min_video_count)

        self.start_offsets_bucket = defaultdict(list)  # each bucket has different batch_size, value is list of video offset start index
        self.bucket_max_seq_count = dict()
        self.video_padding_offset = dict()
        self.batch_size = batch_size
        for bucket_label, video_id_list in self.video_bucket.items():
            # 该桶子最长的视频多少帧
            self.bucket_max_seq_count[bucket_label] = max([(video_id, self.video_count[video_id]) \
                                                          for video_id in video_id_list], key=lambda e:e[1])
            for video_id in video_id_list:
                self.video_padding_offset[video_id] = self.video_offset[video_id] + self.video_count[video_id]
            assert batch_size <= len(video_id_list) # batch_size < video_id_list,桶子内切换文章，一个桶子处理完了切换桶子
            for i in len(video_id_list):
                video_id = video_id_list[i % len(video_id_list)]
                start_offset = self.video_offset[video_id]
                new_offset = start_offset + (i // len(video_id_list) * rest_offset)
                if new_offset >= start_offset + self.video_count[video_id]:
                    new_offset = start_offset
                self.start_offsets_bucket[bucket_label].append((video_id, new_offset)) # value is list of video offset start index

        self.iteration = {bucket_label: 0 for bucket_label in self.video_bucket.keys()}
        self.current_bucket = 0




    def run_prefetch(self):
        manager = Manager()
        self.queue_prefetch = manager.Queue(maxsize=self.n_prefetch)
        self.lock = manager.Lock()
        self.pool = Pool()
        for _ in range(self.n_processes):
            self.pool.apply_async(self._prefetch, args=(self.queue_prefetch, self.lock))
        self.pool.close()
        self.pool.join()
        # self.queue_prefetch.close()
        # self.queue_prefetch.join_thread()


    def _prefetch(self, queue_prefetch, lock):

        while True:
            lock.acquire()
            epoch = self.current_bucket // len(self.video_bucket)
            if not self.repeat and epoch >= len(self.video_bucket):
                lock.release()
                break

            current_backet = self.current_bucket % len(self.video_bucket)
            max_video_id, max_seq_count = self.bucket_max_seq_count[current_backet]
            if self.iteration[current_backet] >= max_seq_count:  # FIXME if max_seq_count == 1, this will cause bug
                self.iteration[current_backet] = 0  # for self.iteration[current_backet] += 1
                self.current_bucket += 1  # forever ++

            self.iteration[current_backet] += 1
            lock.release()
            next_batch = self.get_batch()

            lock.acquire()
            self.is_new_epoch = self.epoch < epoch
            if self.is_new_epoch:
                self.epoch = epoch
            queue_prefetch.put(next_batch, block=True)
            lock.release()


    def __next__(self):
        next_batch = self.queue_prefetch.get(block=True)
        return next_batch

    @property
    def epoch_detail(self):
        return self.current_bucket / len(self.video_bucket)

    def get_batch(self):
        all_frame_in_batch = []
        current_backet = self.current_bucket % len(self.video_bucket)
        max_video_id, max_seq_count = self.bucket_max_seq_count[current_backet]
        offsets = self.start_offsets_bucket[current_backet]
        for video_id, start_offset in offsets:  # len(offsets) = batch_size
            current_offset = start_offset + self.iteration[current_backet] - 1
            # 下面这个if语句，设想如果batch_size > 每个桶子里视频数的时候，会有一些指针的仍有较大的padding
            if current_offset >= self.video_padding_offset[video_id] and video_id != max_video_id:
                all_frame_in_batch.append((np.zeros(config.CHANNEL, config.IMG_SIZE[0], config.IMG_SIZE[1]),
                                           np.array([[0.0,0.0, config.IMG_SIZE[0], config.IMG_SIZE[1]]],dtype=np.float32),
                                           np.zeros(shape=(1, len(config.AU_SQUEEZE)), dtype=np.int32)))
            else:
                try:
                    all_frame_in_batch.append(self.dataset[current_offset])
                except IndexError:
                    all_frame_in_batch.append(self.dataset[current_offset - 1])  # may cause bug?
                    # all_frame_in_batch.append((np.zeros(config.CHANNEL, config.IMG_SIZE[0], config.IMG_SIZE[1]),
                    #                            np.array([[0.0, 0.0, config.IMG_SIZE[0], config.IMG_SIZE[1]]],
                    #                                     dtype=np.float32),
                    #                            np.zeros(shape=(1, len(config.AU_SQUEEZE)), dtype=np.int32)))
        return all_frame_in_batch

    def serialize(self, serializer):
        self.current_bucket = serializer('current_bucket', self.current_bucket)
        self.iteration = serializer('iteration', self.iteration)
        self.epoch = serializer('epoch', self.epoch)

